---
title: "PSL (F20) Coding Assignment 1"
author: "Vijayakumar Sitha Mohan, Matthew Leung"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
library(class)
library(ggplot2)
```

## Introduction


### Intialization


```{r}
# fill the following result during 20 simulation run
linearTestErr = data.frame(method = rep("linear",20), set = rep("test", 20), errorRate = rep(0, 20)  )
linearTrainErr = data.frame(method = rep("linear",20), set = rep("train", 20), errorRate = rep(0, 20)  )

quadTestErr = data.frame(method = rep("quadratic",20), set = rep("test", 20), errorRate = rep(0, 20)  )
quadTrainErr = data.frame(method = rep("quadratic",20), set = rep("train", 20), errorRate = rep(0, 20)  )

knnTestErr = data.frame(method = rep("KNN",20), set = rep("test", 20), errorRate = rep(0, 20)  )
knnTrainErr = data.frame(method = rep("KNN",20), set = rep("train", 20), errorRate = rep(0, 20)  )

bayesTestErr = data.frame(method = rep("bayesRule",20), set = rep("test", 20), errorRate = rep(0, 20)  )
bayesTrainErr = data.frame(method = rep("bayesRule",20), set = rep("train", 20), errorRate = rep(0, 20)  )

KNNCvBestK = rep(0, 20)

```



```{r}
set.seed(8742)
centers = 10; # number of centers
p = 2;      
sd = 1;  # sd for generating the centers within each class
m1 = matrix(rnorm(centers*p), centers, p)*sd + cbind( rep(1,centers), rep(0,centers));
m0 = matrix(rnorm(centers*p), centers, p)*sd + cbind( rep(0,centers), rep(1,centers));

```

```{r}
generate_sim_data= function(centers = 10, n = 100, p = 2, sd = sqrt(1/5) ) {
  # Randomly allocate the n samples for class 1  to the 10 clusters
  id1 = sample(1:centers, n, replace = TRUE);
  # Randomly allocate the n samples for class 1 to the 10 clusters
  id0 = sample(1:centers, n, replace = TRUE);  
  matrix(rnorm(2*n*p), 2*n, p)*sd + rbind(m1[id1,], m0[id0,])
}
```

### Generate Train Data
```{r}
n = 100
train_data = generate_sim_data()
cl_train = factor(c(rep(1,n), rep(0,n)))
train_df = data.frame(train_data)
train_df$y = c(c(rep(1,n), rep(0,n)))
colnames(train_df) = c("x1","x2","y")
```

### Generate Test Data
```{r}
N = 5000
test_data = generate_sim_data(n = N)
cl_test = factor(c(rep(1,N), rep(0,N)))
test_df = data.frame(test_data)
test_df$y = cl_test
colnames(test_df) = c("x1","x2","y")
```

### Visualization

```{r}
par(mfrow = c(1,2))
plot(x2 ~ x1, col = cl_train, data = train_df,
     ylim = c(0, 5), xlim = c(0, 5), pch = 20, cex = 1.5)
plot(x2 ~ x1, col = cl_test, data = test_df,
     ylim = c(0, 6), xlim = c(0, 6), pch = 20, cex = 1.5)
qqplot(train_df$x1,train_df$x2,plot.it = TRUE)
```

```{r}
plot(train_data[, 1], train_data[, 2], type = "n", xlab = "", ylab = "")

points(train_data[1:n, 1], train_data[1:n, 2], col = "blue");
points(train_data[(n+1):(2*n), 1], train_data[(n+1):(2*n), 2], col="red"); 

points(m1[1:10, 1], m1[1:10, 2], pch="+", cex=1.5, col="blue");    
points(m0[1:10, 1], m0[1:10, 2], pch="+", cex=1.5, col="red");   

legend("bottomleft", pch = c(1,1), col = c("red", "blue"), 
       legend = c("class 1", "class 0"))
```


## Method

### 1) Classification based on linear regression

```{r}
lm_fit = lm(y ~ ., data = train_df)
lm_pred_cl_train = as.numeric(lm_fit$fitted > 0.5)
paste("Train Error with Linear Regression Model:",mean(train_df$y != lm_pred_cl_train))
paste("Test Error with Linear Regression Model:",round(mean(ifelse(predict(lm_fit, newdata = test_df, interval = "prediction") > 0.5, 1, 0) != test_df$y),4))
```
```{r}
ggplot(data = train_df, aes(x = x1, y = x2, color = as.factor(y))) + 
    geom_point() + geom_abline(slope = -lm_fit$coefficients[2]/lm_fit$coefficients[3], 
    intercept = (0.5 - lm_fit$coefficients[1])/lm_fit$coefficients[3], 
    size = 1.2)
```


### 2) Classification based on quadratic regression
```{r}
quad_fit = lm(y ~ x1 + x2 + I(x1*x2) + I(x1^2) + I(x2^2), data = train_df)
lm_pred_cl_train = as.numeric(quad_fit$fitted > 0.5)
paste("Train Error with Quadratic Model:",mean(train_df$y != lm_pred_cl_train))
paste("Test Error with Quadratic Model:",round(mean(ifelse(predict(quad_fit, newdata = test_df, interval = "prediction") > 0.5, 1, 0) != test_df$y),4))
```

```{r}
# create a grid of points to be predicted by quad regression
X1 = seq(-5, 5, 0.1)
X2 = seq(-5, 5, 0.1)
X = expand.grid(x1 = X1, x2 = X2)
Y = ifelse(predict(quad_fit, newdata = X) > 0.5, 1, 0)

# plot the area
ggplot(data = X ) +
  geom_point(aes(x = x1, y = x2, color = as.factor(Y)), size = 0.1) +
  geom_contour(aes(x = x1, y = x2, z = Y), color = "grey32", inherit.aes = FALSE) +
  geom_point(data = train_df, aes(x = x1, y = x2, color = as.factor(y)), size = 2) +
  ggtitle("Quadratic Regression ")
  


```


### 3) KNN Classification with K chosen by 10 fold CV
```{r}
set.seed(8742)
dataSet = rbind(train_df,test_df)
dataSet = dataSet[sample(nrow(train_df)),]
foldNum = 10
cvKNNAveErrorRate=function(K,dataSet,foldSize,foldNum) {
  error = 0
  for (runId in 1:foldNum) {
      testSetIndex = ((runId - 1) * foldSize + 1):(ifelse(runId == 
          foldNum, nrow(dataSet), runId * foldSize))
      trainX = dataSet[-testSetIndex, c("x1", "x2")]
      trainY = as.factor(dataSet[-testSetIndex, ]$y)
      testX = dataSet[testSetIndex, c("x1", "x2")]
      testY = as.factor(dataSet[testSetIndex, ]$y)
      predictY = knn(trainX, testX, trainY, K)
      error = error + sum(predictY != testY)
  }
  error = error/nrow(dataSet)
  error
}

cvKNN = function(dataSet, foldNum) {
    foldSize = floor(nrow(dataSet)/foldNum)
    KVector = seq(1, (nrow(dataSet) - foldSize), 2)
    cvKNNAveErrorRates = sapply(KVector, cvKNNAveErrorRate, dataSet, 
        foldSize, foldNum)
    result = list()
    result$bestK = max(KVector[cvKNNAveErrorRates == min(cvKNNAveErrorRates)])
    bestK_index = which(KVector %in% c(result$bestK))
    result$cvError = cvKNNAveErrorRates[bestK_index]
    result
}

cvKNN(dataSet,foldNum)

```

```{r}
# create a grid of points to be predicted by KNN
X1 = seq(-5, 5, 0.1)
X2 = seq(-5, 5, 0.1)
X = expand.grid(x1 = X1, x2 = X2)
Y = knn(train_df[, c("x1", "x2")], X, train_df[, c("y")] , 13) 

# plot the area
ggplot(data = X ) +
  geom_point(aes(x = x1, y = x2, color = as.factor(Y)), size = 0.1) +
  geom_contour(aes(x = x1, y = x2, z = Y), color = "grey32", inherit.aes = FALSE) +
  geom_point(data = train_df, aes(x = x1, y = x2, color = as.factor(y)), size = 2) +
  ggtitle("KNN with K = 13 ")
  



```



### 4) Classification based on Bayes Rules

```{r}
# bayes error
mixedModelBayes=function(x){
  sum(exp(-apply((t(m1)-x)^2, 2, sum)*5/2))/sum(exp(-apply((t(m0)-x)^2, 2, sum)*5/2))
}


cl_test_pred_Bayes = apply(test_data, 1, mixedModelBayes)
cl_test_pred_Bayes = as.numeric(cl_test_pred_Bayes > 1);
table(cl_test, cl_test_pred_Bayes); 
test.err.Bayes = sum(cl_test !=  cl_test_pred_Bayes) / (2*N)
print(test.err.Bayes)
```

```{r}

# create a grid of points to be predicted by bayes rule
X1 = seq(-5, 5, 0.1)
X2 = seq(-5, 5, 0.1)
X = expand.grid(x1 = X1, x2 = X2)
Y = as.numeric(apply(X, 1, mixedModelBayes) > 1)

# plot the area
ggplot(data = X ) +
  geom_point(aes(x = x1, y = x2, color = as.factor(Y)), size = 0.1) +
  geom_contour(aes(x = x1, y = x2, z = Y), color = "grey32", inherit.aes = FALSE) +
  geom_point(data = train_df, aes(x = x1, y = x2, color = as.factor(y)), size = 2) +
  ggtitle("Classification based on the Bayes Rule ")


```

## Results

```{r}

# consolidate all results
finalErr = rbind(linearTestErr, linearTrainErr, quadTestErr, quadTrainErr, knnTestErr, knnTrainErr, bayesTestErr, bayesTrainErr)

# randomly assign some error for visualization testing
# comment this out when real data is available
finalErr$errorRate = runif(nrow(finalErr), min = 0.01, max = 0.9)

# plot boxplot
ggplot(finalErr, aes(x = method, y = errorRate, fill = set)) +
  geom_boxplot() +
  ggtitle("Repeat simulation 20 times and compare the performance for all methods ")
```

```{r}

# randomly assign best K for visualization testing
# comment this out when real data is available
KNNCvBestK = floor(runif(length(KNNCvBestK), min = 1, max = 100))

bestK_df = data.frame(simulation = seq(1,20,1), K = KNNCvBestK)

# plot barplot for 20 best K
ggplot(bestK_df, aes(x = simulation, y = K)) +
  geom_bar(stat = "identity") +
  ggtitle("best K selected by 10 fold cross validation for KNN ")



```


Report the mean and sd of selected K values

```{r}
mean(KNNCvBestK)

```


```{r}
sd(KNNCvBestK)
```



## Discussion



