---
title: "PSL (F20) Coding Assignment 1"
author: "Vijayakumar Sitha Mohan (VS24), Waitong Matthew Leung (wmleung2)"
date: '6-Sept-2020'
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
library(class)
library(ggplot2)
```

## Introduction

This assignment is related to the simulation study described in Section 2.3.1 (the so-called Scenario 2) of "Elements of Statistical Learning" (ESL).

Scenario 2: the two-dimensional data X in R2 in each class is generated from a
mixture of 10 different bivariate Gaussian distributions with uncorrelated components and different means.

The study repeats the following simulation 20 times. In each simulation,

1. follow the data generating process to generate a training sample of size 200 and a test sample of size 10,000, and

2. calculate the training and test errors (the averaged 0/1 error )
for each the following four procedures:

- Linear regression with cut-off value 0.5,
- quadratic regression with cut-off value 0.5,
- kNN classification with k chosen by 10-fold cross-validation, and
- the Bayes rule (assume your know the values of mkl's and s).

### Intialization

- Initialize variables to hold training and test errors for selected 4 procedures.

```{r}
# fill the following result during 20 simulation run
linearTestErr = data.frame(method = rep("linear",20), set = rep("test", 20), errorRate = rep(0.0, 20)  )
linearTrainErr = data.frame(method = rep("linear",20), set = rep("train", 20), errorRate = rep(0.0, 20)  )

quadTestErr = data.frame(method = rep("quadratic",20), set = rep("test", 20), errorRate = rep(0.0, 20)  )
quadTrainErr = data.frame(method = rep("quadratic",20), set = rep("train", 20), errorRate = rep(0.0, 20)  )

knnTestErr = data.frame(method = rep("KNN",20), set = rep("test", 20), errorRate = rep(0.0, 20) )
knnTrainErr = data.frame(method = rep("KNN",20), set = rep("train", 20), errorRate = rep(0.0, 20)  )

bayesTestErr = data.frame(method = rep("bayesRule",20), set = rep("test", 20), errorRate = rep(0.0, 20)  )
bayesTrainErr = data.frame(method = rep("bayesRule",20), set = rep("train", 20), errorRate = rep(0., 20)  )
KNNCvBestK = rep(0, 20)
set.seed(8742)
```

- Generate 20 centers randomly

```{r}
generateCenters = function() {
  centers = 10; # number of centers
  p = 2;    
  sd = 1;  # sd for generating the centers within each class
  m1 = matrix(rnorm(centers*p), centers, p)*sd + cbind( rep(1,centers), rep(0,centers));
  m0 = matrix(rnorm(centers*p), centers, p)*sd + cbind( rep(0,centers), rep(1,centers));
  list(m0,m1)
}
```

- Function to generate data with a size

```{r}
generate_sim_data= function(centers = 10, n = 100, p = 2, sd = sqrt(1/5) ) {

  centers_mat = generateCenters()
  m0 = centers_mat[[1]]
  m1 = centers_mat[[2]]
  # Randomly allocate the n samples for class 1  to the 10 clusters
  id1 = sample(1:centers, n, replace = TRUE);
  # Randomly allocate the n samples for class 1 to the 10 clusters
  id0 = sample(1:centers, n, replace = TRUE);  
  matrix(rnorm(2*n*p), 2*n, p)*sd + rbind(m1[id1,], m0[id0,])
}
```

- Generate Training and Testing data

```{r}
generateTrainAndTestData = function(train = 100, test = 5000) {

  train_data = generate_sim_data()
  train_df = data.frame(train_data)
  train_df$y = c(c(rep(1,train), rep(0,train)))
  colnames(train_df) = c("x1","x2","y")
  
  test_data = generate_sim_data(n = test)
  test_df = data.frame(test_data)
  test_df$y = c(rep(1,test), rep(0,test))
  colnames(test_df) = c("x1","x2","y")
  list(train_data = train_df, test_data = test_df)
}
```

### Visualization

- Visualize generated data in a scatter plot

```{r}
sim_data = generateTrainAndTestData()
train_df = sim_data$train_data
test_df = sim_data$test_data
centers_mat = generateCenters()
m0 = centers_mat[[1]]
m1 = centers_mat[[2]]
```

```{r}
#par(mfrow = c(1,2))
#plot(x2 ~ x1, col = y, data = train_df,
#     ylim = c(0, 5), xlim = c(0, 5), pch = 20, cex = 1.5)
#plot(x2 ~ x1, col = y, data = test_df,
#     ylim = c(0, 6), xlim = c(0, 6), pch = 20, cex = 1.5)
```

```{r}
plot(train_df$x1, train_df$x2, type = "n", xlab = "", ylab = "")

points(train_df[train_df$y == 1,1], train_df[train_df$y == 1,2], col = "blue");
points(train_df[train_df$y == 0,1], train_df[train_df$y == 0,2], col = "red"); 

points(m1[1:10, 1], m1[1:10, 2], pch="+", cex=1.5, col="blue");    
points(m0[1:10, 1], m0[1:10, 2], pch="+", cex=1.5, col="red");   

legend("bottomleft", pch = c(1,1), col = c("red", "blue"), 
       legend = c("class 1", "class 0"))
```


## Method

### 1) Classification based on linear regression

```{r}
simpleLinearRegression = function(index) {
  lm_fit = lm(y ~ ., data = train_df)
  lm_pred_cl_train = as.numeric(lm_fit$fitted > 0.5)
  c(train_error = mean(train_df$y != lm_pred_cl_train),
  test_error = round(mean(ifelse(predict(lm_fit, newdata = test_df) > 0.5, 1, 0) != test_df$y),4))
}
```

```{r}
linearPlot = function (lm_fit) {
  ggplot(data = train_df, aes(x = x1, y = x2, color = as.factor(y))) + 
      geom_point() + geom_abline(slope = -lm_fit$coefficients[2]/lm_fit$coefficients[3], 
      intercept = (0.5 - lm_fit$coefficients[1])/lm_fit$coefficients[3], 
      size = 1.2) +
    geom_point(data = data.frame(m1), aes(x = X1, y = X2, color = as.factor(1)), shape  = 9, size = 4) + 
    geom_point(data = data.frame(m0), aes(x = X1, y = X2, color = as.factor(0)), shape  = 9, size = 4) +
    ggtitle("Linear Regression ")
}
```


### 2) Classification based on quadratic regression

```{r}
quadraticRegression = function(index) {
  quad_fit = lm(y ~ x1 + x2 + I(x1*x2) + I(x1^2) + I(x2^2), data = train_df)
  lm_pred_cl_train = as.numeric(quad_fit$fitted > 0.5)
  c(train_error = mean(train_df$y != lm_pred_cl_train),
  test_error  = round(mean(ifelse(predict(quad_fit, newdata = test_df) > 0.5, 1, 0) != test_df$y),4))
}
```

```{r}
quadraticPlot = function(quad_fit) {
# create a grid of points to be predicted by quad regression
X1 = seq(-3, 4, 0.1)
X2 = seq(-3, 4, 0.1)
X = expand.grid(x1 = X1, x2 = X2)
Y = ifelse(predict(quad_fit, newdata = X) > 0.5, 1, 0)

# plot the area
ggplot(data = X ) +
  geom_point(aes(x = x1, y = x2, color = as.factor(Y)), size = 0.1) +
  geom_contour(aes(x = x1, y = x2, z = Y), color = "grey32", inherit.aes = FALSE) +
  geom_point(data = train_df, aes(x = x1, y = x2, color = as.factor(y)), size = 2) +
    geom_point(data = data.frame(m1), aes(x = X1, y = X2, color = as.factor(1)), shape  = 9, size = 4) + 
  geom_point(data = data.frame(m0), aes(x = X1, y = X2, color = as.factor(0)), shape  = 9, size = 4) +
  ggtitle("Quadratic Regression ")
  
}

```


### 3) KNN Classification with K chosen by 10 fold CV

```{r}

getCvKNNAveErrorRate=function(K,dataSet,foldSize,foldNum) {
  error = 0
  for (runId in 1:foldNum) {
      testSetIndex = ((runId - 1) * foldSize + 1):(ifelse(runId == 
          foldNum, nrow(dataSet), runId * foldSize))
      trainX = dataSet[-testSetIndex, c("x1", "x2")]
      trainY = as.factor(dataSet[-testSetIndex, ]$y)
      testX = dataSet[testSetIndex, c("x1", "x2")]
      testY = as.factor(dataSet[testSetIndex, ]$y)
      predictY = knn(trainX, testX, trainY, K)
      error = error + sum(predictY != testY)
  }
  error = error/nrow(dataSet)
  error
}

cvKNN = function(dataSet, foldNum) {
    foldSize = floor(nrow(dataSet)/foldNum)
    KVector = seq(1, (nrow(dataSet) - foldSize), 2)
    cvKNNAveErrorRates = sapply(KVector, getCvKNNAveErrorRate, dataSet, 
        foldSize, foldNum)
    result = list()
    result$bestK = max(KVector[cvKNNAveErrorRates == min(cvKNNAveErrorRates)])
    bestK_index = which(KVector %in% c(result$bestK))
    
    result$train.cvError = sum(knn(train_df[,c("x1", "x2")], train_df[,c("x1", "x2")], train_df$y, k = result$bestK) != train_df$y) / nrow(train_df)
    
    predictY = knn(train_df[,c("x1", "x2")], test_df[,c("x1", "x2")], train_df$y, k = result$bestK)
    result$cvError = sum(predictY != test_df$y) / nrow(test_df)
    result
}

foldNum = 10
kNearestNeighbor = function(index) {
  dataSet = train_df[sample(nrow(train_df)),]
  result = cvKNN(dataSet,foldNum)
  c(result$train.cvError,result$cvError,result$bestK)
}
```

```{r}
knnPlot = function(bestK) {
  # create a grid of points to be predicted by KNN
  X1 = seq(-3, 4, 0.1)
  X2 = seq(-3, 4, 0.1)
  X = expand.grid(x1 = X1, x2 = X2)
  Y = knn(train_df[, c("x1", "x2")], X, train_df[, c("y")] , bestK) 
  
  # plot the area
  ggplot(data = X ) +
    geom_point(aes(x = x1, y = x2, color = as.factor(Y)), size = 0.1) +
    geom_contour(aes(x = x1, y = x2, z = as.integer(Y)), color = "grey32", inherit.aes = FALSE) +
    geom_point(data = train_df, aes(x = x1, y = x2, color = as.factor(y)), size = 2) +
      geom_point(data = data.frame(m1), aes(x = X1, y = X2, color = as.factor(1)), shape  = 9, size = 4) + 
    geom_point(data = data.frame(m0), aes(x = X1, y = X2, color = as.factor(0)), shape  = 9, size = 4) +
    ggtitle(paste("KNN with K = ", bestK))
  
}


```



### 4) Classification based on Bayes Rules

```{r}
# bayes error
mixedModelBayes=function(x){
  sum(exp(-apply((t(m1)-x)^2, 2, sum)*5/2))/sum(exp(-apply((t(m0)-x)^2, 2, sum)*5/2))
}

bayesError = function(index) {
  cl_train_pred_Bayes = apply(train_df[,1:2], 1, mixedModelBayes)
  cl_train_pred_Bayes = as.numeric(cl_train_pred_Bayes > 1);
  train.err.Bayes = sum(train_df$y !=  cl_train_pred_Bayes) / nrow(train_df)

  cl_test_pred_Bayes = apply(test_df[,1:2], 1, mixedModelBayes)
  cl_test_pred_Bayes = as.numeric(cl_test_pred_Bayes > 1);
  test.err.Bayes = sum(test_df$y !=  cl_test_pred_Bayes) / nrow(test_df)
  c(train.err.Bayes,test.err.Bayes)
}
```

```{r}
bayesPlot = function() {
  # create a grid of points to be predicted by bayes rule
  X1 = seq(-3, 4, 0.1)
  X2 = seq(-3, 4, 0.1)
  X = expand.grid(x1 = X1, x2 = X2)
  Y = as.numeric(apply(X, 1, mixedModelBayes) > 1)
  
  # plot the area
  ggplot(data = X ) +
    geom_point(aes(x = x1, y = x2, color = as.factor(Y)), size = 0.1) +
    geom_contour(aes(x = x1, y = x2, z = Y), color = "grey32", inherit.aes = FALSE) +
    geom_point(data = train_df, aes(x = x1, y = x2, color = as.factor(y)), size = 2) +
      geom_point(data = data.frame(m1), aes(x = X1, y = X2, color = as.factor(1)), shape  =9, size = 4) + 
    geom_point(data = data.frame(m0), aes(x = X1, y = X2, color = as.factor(0)), shape  = 9, size = 4) +
    ggtitle("Classification based on the Bayes Rule ")
}

```

## Results

```{r message=FALSE, warning=FALSE}
simSize = 20

for(i in 1:simSize) {
  sim_data = generateTrainAndTestData()
  train_df = sim_data$train_data
  test_df = sim_data$test_data
  lr_errors = simpleLinearRegression(i)
  linearTrainErr[i,"errorRate"] = lr_errors[1]
  linearTestErr[i,"errorRate"] = lr_errors[2]
  quad_errors = quadraticRegression(i)
  quadTrainErr[i,"errorRate"] = quad_errors[1]
  quadTestErr[i,"errorRate"] = quad_errors[2]
  knn_errors = kNearestNeighbor(i)
  knnTrainErr[i,"errorRate"] = knn_errors[1]
  knnTestErr[i,"errorRate"] = knn_errors[2]
  KNNCvBestK[i] = knn_errors[3]
  bayes_errors = bayesError(i)
  bayesTrainErr[i,"errorRate"] = bayes_errors[1]
  bayesTestErr[i,"errorRate"] = bayes_errors[2]
}

# visualization of 4 methods
linearPlot( lm(y ~ ., data = train_df) )
quadraticPlot( lm(y ~ x1 + x2 + I(x1*x2) + I(x1^2) + I(x2^2), data = train_df) )
knnPlot(min(KNNCvBestK))
bayesPlot()

# consolidate all results
finalErr = rbind(linearTestErr, linearTrainErr, quadTestErr, quadTrainErr, knnTestErr, knnTrainErr, bayesTestErr, bayesTrainErr)

# plot boxplot
ggplot(finalErr, aes(x = method, y = errorRate, fill = set)) +
  geom_boxplot() +
  ggtitle("Repeat simulation 20 times and compare the performance for all methods ")
```

```{r}

bestK_df = data.frame(simulation = seq(1,20,1), K = KNNCvBestK)

# plot barplot for 20 best K
ggplot(bestK_df, aes(x = simulation, y = K)) +
  geom_bar(stat = "identity", fill = rainbow(20)) +
  ggtitle("best K selected by 10 fold cross validation for KNN ")

```


Report the mean and sd of selected K values:

```{r}
mean(KNNCvBestK)

```


```{r}
sd(KNNCvBestK)
```



## Discussion



## Contribution

Vijayakumar Mohan contributes to overall factoring programming structure and efficiency to cater to the need of this assignment repeating 4 classification methods for 20 times. 

Matthew Leung contributes to visualization, verification of calculation and overall presentation of the report.