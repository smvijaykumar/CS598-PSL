---
title: "PSL (F20) Coding Assignment 1"
author: "Vijayakumar Sitha Mohan, Matthew Leung"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
library(class)
library(ggplot2)
```

## Introduction


### Intialization

```{r}
set.seed(8742)
centers = 10; # number of centers
p = 2;      
sd = 1;  # sd for generating the centers within each class
m1 = matrix(rnorm(centers*p), centers, p)*sd + cbind( rep(1,centers), rep(0,centers));
m0 = matrix(rnorm(centers*p), centers, p)*sd + cbind( rep(0,centers), rep(1,centers));

```

```{r}
generate_sim_data= function(centers = 10, n = 100, p = 2, sd = sqrt(1/5) ) {
  # Randomly allocate the n samples for class 1  to the 10 clusters
  id1 = sample(1:centers, n, replace = TRUE);
  # Randomly allocate the n samples for class 1 to the 10 clusters
  id0 = sample(1:centers, n, replace = TRUE);  
  matrix(rnorm(2*n*p), 2*n, p)*sd + rbind(m1[id1,], m0[id0,])
}
```

### Generate Train Data
```{r}
n = 100
train_data = generate_sim_data()
cl_train = factor(c(rep(1,n), rep(0,n)))
train_df = data.frame(train_data)
train_df$y = c(c(rep(1,n), rep(0,n)))
colnames(train_df) = c("x1","x2","y")
```

### Generate Test Data
```{r}
N = 5000
test_data = generate_sim_data(n = N)
cl_test = factor(c(rep(1,N), rep(0,N)))
test_df = data.frame(test_data)
test_df$y = cl_test
colnames(test_df) = c("x1","x2","y")
```

### Visualization

```{r}
par(mfrow = c(1,2))
plot(x2 ~ x1, col = cl_train, data = train_df,
     ylim = c(0, 5), xlim = c(0, 5), pch = 20, cex = 1.5)
plot(x2 ~ x1, col = cl_test, data = test_df,
     ylim = c(0, 6), xlim = c(0, 6), pch = 20, cex = 1.5)
qqplot(train_df$x1,train_df$x2,plot.it = TRUE)
```

```{r}
plot(train_data[, 1], train_data[, 2], type = "n", xlab = "", ylab = "")

points(train_data[1:n, 1], train_data[1:n, 2], col = "blue");
points(train_data[(n+1):(2*n), 1], train_data[(n+1):(2*n), 2], col="red"); 

points(m1[1:10, 1], m1[1:10, 2], pch="+", cex=1.5, col="blue");    
points(m0[1:10, 1], m0[1:10, 2], pch="+", cex=1.5, col="red");   

legend("bottomleft", pch = c(1,1), col = c("red", "blue"), 
       legend = c("class 1", "class 0"))
```


## Method

### 1) Classification based on linear regression

```{r}
lm_fit = lm(y ~ ., data = train_df)
lm_pred_cl_train = as.numeric(lm_fit$fitted > 0.5)
paste("Train Error with Linear Regression Model:",mean(train_df$y != lm_pred_cl_train))
paste("Test Error with Linear Regression Model:",round(mean(ifelse(predict(lm_fit, newdata = test_df, interval = "prediction") > 0.5, 1, 0) != test_df$y),4))
```
```{r}
ggplot(data = train_df, aes(x = x1, y = x2, color = as.factor(y))) + 
    geom_point() + geom_abline(slope = -lm_fit$coefficients[2]/lm_fit$coefficients[3], 
    intercept = (0.5 - lm_fit$coefficients[1])/lm_fit$coefficients[3], 
    size = 1.2)
```


### 2) Classification based on quadratic regression
```{r}
quad_fit = lm(y ~ x1 + x2 + I(x1*x2) + I(x1^2) + I(x2^2), data = train_df)
lm_pred_cl_train = as.numeric(quad_fit$fitted > 0.5)
paste("Train Error with Quadratic Model:",mean(train_df$y != lm_pred_cl_train))
paste("Test Error with Quadratic Model:",round(mean(ifelse(predict(quad_fit, newdata = test_df, interval = "prediction") > 0.5, 1, 0) != test_df$y),4))
```



### 3) KNN Classification with K chosen by 10 fold CV
```{r}
set.seed(8742)
dataSet = rbind(train_df,test_df)
dataSet = dataSet[sample(nrow(train_df)),]
foldNum = 10
cvKNNAveErrorRate=function(K,dataSet,foldSize,foldNum) {
  error = 0
  for (runId in 1:foldNum) {
      testSetIndex = ((runId - 1) * foldSize + 1):(ifelse(runId == 
          foldNum, nrow(dataSet), runId * foldSize))
      trainX = dataSet[-testSetIndex, c("x1", "x2")]
      trainY = as.factor(dataSet[-testSetIndex, ]$y)
      testX = dataSet[testSetIndex, c("x1", "x2")]
      testY = as.factor(dataSet[testSetIndex, ]$y)
      predictY = knn(trainX, testX, trainY, K)
      error = error + sum(predictY != testY)
  }
  error = error/nrow(dataSet)
  error
}

cvKNN = function(dataSet, foldNum) {
    foldSize = floor(nrow(dataSet)/foldNum)
    KVector = seq(1, (nrow(dataSet) - foldSize), 2)
    cvKNNAveErrorRates = sapply(KVector, cvKNNAveErrorRate, dataSet, 
        foldSize, foldNum)
    result = list()
    result$bestK = max(KVector[cvKNNAveErrorRates == min(cvKNNAveErrorRates)])
    bestK_index = which(KVector %in% c(result$bestK))
    result$cvError = cvKNNAveErrorRates[bestK_index]
    result
}

cvKNN(dataSet,foldNum)

```

### 4) Classification based on Bayes Rules

```{r}
# bayes error
mixedModelBayes=function(x){
  sum(exp(-apply((t(m1)-x)^2, 2, sum)*5/2))/sum(exp(-apply((t(m0)-x)^2, 2, sum)*5/2))
}


cl_test_pred_Bayes = apply(test_data, 1, mixedModelBayes)
cl_test_pred_Bayes = as.numeric(cl_test_pred_Bayes > 1);
table(cl_test, cl_test_pred_Bayes); 
test.err.Bayes = sum(cl_test !=  cl_test_pred_Bayes) / (2*N)
print(test.err.Bayes)
```


## Results

## Discussion


