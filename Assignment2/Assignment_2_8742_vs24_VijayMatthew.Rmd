---
title: "PSL (F20) Coding Assignment 2"
author: "Vijayakumar Sitha Mohan (VS24), Waitong Matthew Leung (wmleung2)"
date: '13-Sept-2020'
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
set.seed(8742)
```

## Introduction

Implement Lasso using the Coordinate Descent (CD) algorithm and apply our algorithm
on the Boston housing data. The goal is to check the accuracy of our implementation compare to the output of glmnet.The maximum difference between the two coefficient matrices should be less than 0.005.


## Data Preparation

Load required R packages and apply proper transformations on the Boston Housing Data.

```{r}

library(MASS)
library(glmnet)
myData = Boston
names(myData)[14] = "Y"
iLog = c(1, 3, 5, 6, 8, 9, 10, 14);
myData[, iLog] = log(myData[, iLog]);
myData[, 2] = myData[, 2] / 10;
myData[, 7] = myData[, 7]^2.5 / 10^4
myData[, 11] = exp(0.4 * myData[, 11]) / 1000;
myData[, 12] = myData[, 12] / 100;
myData[, 13] = sqrt(myData[, 13]);
X = as.matrix(myData[, -14])
y = myData$Y
lam.seq = c(0.30, 0.2, 0.1, 0.05, 0.02, 0.005)

```


```{r}
pairs(myData)
```


## CD for Lasso

Implement the Coordinate Descent algorithm for Lasso in the function MyLasso here.

```{r}

# function to solve the Lasso estimate for Î²j given other coefficients fixed
one_var_lasso = function(r, x, lam) {
    xx = sum(x^2)
    xr = sum(r * x)
    b = (abs(xr) - lam/2)/xx
    b = sign(xr) * ifelse(b > 0, b, 0)
    return(b)
}


MyLasso = function(X, y, lam.seq, maxit = 50) {
    
    # X: n-by-p design matrix without the intercept 
    # y: n-by-1 response vector 
    # lam.seq: sequence of lambda values 
    # maxit: number of updates for each lambda 
    # Center/Scale X
    # Center y
  
    n = length(y)
    p = dim(X)[2]
    nlam = length(lam.seq)
  
    ##############################
    # YOUR CODE: 
    # Record the corresponding means and scales
    # For example, 
    # y.mean = mean(y)
    # yc = centered y
    # Xs = centered and scaled X
    
    y.mean = mean(y)
    yc = y - y.mean
    
    X.mean = apply(X, 2, mean)
    X.sd = sqrt(apply((X - rep(X.mean, rep.int(nrow(X), ncol(X))))^2, 2, mean))
    Xs = (X - rep(X.mean, rep.int(nrow(X), ncol(X)))) / rep(X.sd, rep.int(nrow(X), ncol(X)))
    
    ############################## 
    
    

    # Initilize coef vector b and residual vector r
    b = rep(0, p)
    r = yc
    B = matrix(nrow = nlam, ncol = p + 1)
    
    # Triple nested loop
    for (m in 1:nlam) {
        lam = 2 * n * lam.seq[m]
        for (step in 1:maxit) {
            for (j in 1:p) {
                r = r + (Xs[, j] * b[j])
                b[j] = one_var_lasso(r, Xs[, j], lam)
                r = r - Xs[, j] * b[j]
            }
        }
        B[m, ] = c(0, b)
    }
   
    ##############################
    # YOUR CODE:
    # Scale back the coefficients;
    # Update the intercepts stored in B[, 1]

    B[, 1] = y.mean - apply(B[,2:(p+1)] * rep(X.mean/X.sd, rep.int(nlam, p)), 1, sum)
    B[, 2:(p+1)] = B[, 2:(p+1)]/ rep(X.sd, rep.int(nlam, p))
    t(B)
    ##############################
}

```



## Results : Check the Accuracy

```{r}

lam.seq = c(0.30 , 0.2 , 0.1 , 0.05 , 0.02 , 0.005)
lasso.fit = glmnet (X, y, alpha = 1, lambda = lam.seq)
coef ( lasso.fit )

myout = MyLasso(X, y, lam.seq , maxit = 100)
rownames(myout) = c("Intercept", colnames(X))
myout


```

Compare the accuracy of my algorithm against the output from glmnet. The maximum difference between the two coefficient matrices is less than 0.005.

```{r}
max(abs(coef(lasso.fit) - myout))

```

```{r}
plot(lasso.fit, xvar = "lambda", label=TRUE)
# Now the l1-norm of the coefficients decreases as lambda increases
plot(lasso.fit$lambda, colSums(abs(lasso.fit$beta)), type = "l",
     xlab = "lambda", ylab = "l1 norm")
```


## Discussion

We are able to replicate glmnet Lasso regularization Coordinate Decent (CD) algorithm on Housing Values in Suburbs of Boston dataset. From the visualization of coefficient Vs Lambda plot, we can see as lambda increases, more and more coefficients becomes zero. lstat remains non-zero at high lambda value. This is consistent with what we see in the pair plot that response medv has high negative correlations with lstat. 

## Contribution

Vijayakumar Mohan contributes to verification of algorithm implementation and visualization.

Matthew Leung contributes to algorithm implementation and overall report presentation.
