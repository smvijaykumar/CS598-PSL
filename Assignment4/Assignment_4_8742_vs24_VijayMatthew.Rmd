---
title: "PSL (F20) Coding Assignment 4"
author: "Vijayakumar Sitha Mohan (VS24), Waitong Matthew Leung (wmleung2)"
date: '3-Nov-2020'
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
options(scipen = 1, digits = 7, width = 80, fig.align = "center")
set.seed(8742)
library(mclust)
```

## Introduction

Implement the EM algorithm for a p-dimensional Gaussian mixture model with G components:

$\sum_{k=1}^{G} P_i * N(x; \mu_k, \Sigma)$

Store the estimated parameters as a list in R with three components

- prob: G-dimensional probability vector (p1,...,pG)

- mean: p-by-G matrix with the k-th column being $\mu_k$, the p-dimensional mean for the
k-th Gaussian component;

- Sigma: p-by-p covariance matrix $\Sigma$ shared by all G components.


## Prepare my function

We prepare a function to perform the E-step, a function to perform the M-step, and then iteratively call these two functions in myEM.



```{r}

##################################################################
# Public functions
##################################################################
#' Multivariate Normal Density 
#' 
#' Calculates mutlivariate normal densities. This is different to the dmvnorm function 
#' in the mvtnorm package in that it takes a matrix for both x and mean. It then calculates 
#' a vector of densities according to dmvnorm(x[i,],mean[i,],sigma,log = FALSE).
#' To aid computation the mahalanobis distances are calculated in parallel using mclapply.
#' @importFrom parallel mclapply
#' @param x A matrix of values
#' @param mean A matrix of means
#' @param sigma A covariance matrix
#' @param log Boolean for whether we want log densities or not
#' @details My own implementation of the multivariate normal density function
#' for increased efficiency for application in this package
#' because there are so many repeated calls to densitymvnorm using a 
#' given sigma matrix it made sense to have one that could take a matrix 
#' of means as well as a matrix of x's and treat them as 
#' paired, returning the density of x[1,] given mean[1,], x[2,] 
#' given mean[2,] also stops repeated inversions of the matrix sigma, and
#' calculates the densities in parallel
#' @return A vector of densities
#' @export
densityMvNorm = function (x, mean, sigma, log = FALSE) 
{
  ## takes a matrix of means rather than a single vector
  if (missing(sigma)) sigma = diag(ncol(x))
  
  if (NCOL(x) != NCOL(sigma)) {
    stop("x and sigma have non-conforming size")
  }
  if (!isSymmetric(sigma, tol = sqrt(.Machine$double.eps), 
                   check.attributes = FALSE)) {
    stop("sigma must be a symmetric matrix")
  }
  if (NCOL(mean) != NROW(sigma)) {
    stop("mean and sigma have non-conforming size")
  }
  ## invert matrix before hand so only do this once
  prec = solve(sigma)
  means = lapply(1:dim(mean)[1],function(i){mean[i,]})
  #distval = do.call(rbind, 
  #                   mclapply(means, 
  #                            mahalanobis,x = x, cov = prec,inverted = TRUE))
  distval = do.call(rbind, 
                     lapply(means, 
                              mahalanobis,x = x, cov = prec,inverted = TRUE))
  logdet = sum(log(eigen(sigma, symmetric = TRUE, only.values = TRUE)$values))
  logretval = -(ncol(x) * log(2 * pi) + logdet + distval)/2
  
  if (log) 
    return(logretval)
  else 
    return(exp(logretval))
}
```



```{r}

Estep <- function(data, G, para){
  # Your Code
  # Return the n-by-G probability matrix
  
  # initialize pre_r = pi_k * N(x_i|mu_k, sigma) with n x G zeros
  pre_r = matrix(0, nrow(data), G)
  
  # loop through each data point i
  for (i in 1:nrow(data)){
    # loop through each class c
    for (c in 1:G){
      pre_r[i,c] = para$prob[c] * densityMvNorm(data[i,], t(as.matrix(para$mean[,c])), para$Sigma)
    }#c      
  }#i
  
  # calculate responsibility r
  r = matrix(0, nrow(data), G)
  r = pre_r/apply(pre_r, 1, sum)

  # return responsibility r
  r
  
}

Mstep <- function(data, G, para, post.prob){ 
  # Your Code
  # Return the updated parameters

  # update prob
  para$prob = apply(post.prob, 2, mean)
  
  
  mySigma = matrix(0,G,G)
  # loop through all class
  for (c in 1:G){

    # update mean
    para$mean[,c] = apply(data * post.prob[,c], 2, sum)/ sum(post.prob[,c])
       
    # calculate sigma
    # subtract class mean from data and convert to matrix for each data i
    for (i in 1:nrow(data)){
      myDatum = as.matrix(data[i,] - para$mean[,c])
      mySigma = mySigma + post.prob[i,c] * t(myDatum) %*% myDatum
    }#i
    
  }#G
  
  # update Sigma
  para$Sigma = mySigma/nrow(data)
  

  para  
}

myEM <- function(data, itmax, G, para){
  # itmax: num of iterations
  # G:     num of components
  # para:  list of parameters (prob, mean, Sigma)
  for(t in 1:itmax){
    post.prob <- Estep(data, G, para)
    para <- Mstep(data, G, para, post.prob)
  }
  return(para)
}


```



## Test my function

Test my function with data faithful from mclust.


### Load data

```{r}

library(mclust)
dim(faithful)


```


```{r}
head(faithful)

```

```{r}
n <- nrow(faithful)

```


### Initialization

The mixture model in this assignment correponds to modelName = "EEE" in mclust. We initialize parameters by first randomly assigning the n samples into two groups and then running one iteration of the built-in M-step.

```{r}

set.seed(8742)
Z <- matrix(0, n, 2) 
Z[sample(1:n, 120), 1] <- 1 
Z[, 2] <- 1 - Z[, 1]
ini0 <- mstep(modelName="EEE", faithful , Z)$parameters

```

Here are the initial values we use for (prob, mean, Sigma).


```{r}

para0 <- list(prob = ini0$pro, 
              mean = ini0$mean, 
              Sigma = ini0$variance$Sigma)
para0

```



### Compare Results

Compare the estimated parameters returned by myEM and the ones returned by the EM algorithm in mclust after 10 iterations.

- Output from myEM


```{r}

myEM(data=faithful, itmax=10, G=2, para=para0)

```


- Output from mclust


```{r}

Rout <- em(modelName = "EEE", data = faithful,
           control = emControl(eps=0, tol=0, itmax = 10), 
           parameters = ini0)$parameters

list(Rout$pro, Rout$mean, Rout$variance$Sigma)

```

## Derivation of the E- and M-steps

Density function of Gaussian mixture model which is a combination of pdf's:

$P(x) = \sum_{k=1}^{K} \pi_k P(x|k)$

$P(x|\theta) = \sum_{k=1}^{K} \pi_k N(x|\mu_k, \Sigma)$

where

- $\theta$ = { $\pi_k, \mu_k, \sigma$} are the parameters
- $\pi_k$ is the mixing weights of mixture component k, and satisfy $0 <= \pi_k <= 1$ and $\sum_{i=1}^{k}\pi_k = 1$ \
- $\mu_k$ is mean of mixture component k \
- $\Sigma$ is covariance that share among all K components
- K is number of components \


The **incomplete data log likelihood** which is the objective function we aim to maximize:

$l(\theta) = logP(x_{1:N}| \theta)$ \
          $=\sum_n logP(x_n | \theta)$ \
          $=\sum_n log \sum_k \pi_k N(x_n|\mu_k, \Sigma)$ \
          
This expression is difficult to solve for optimum point because of the summation inside log function.
          
Let's try EM algorithm which assumes each data point $x_n$ has an associated latent indicator variable $Z_n$ belongs to {1,...,K} indicates which component the data point comes from. We can write the **complete data log likelihood** as:

$l_c(\theta) = logP(x_{1:N}, z_{1:N}| \theta)$ \
            $= \sum_n logP(x_n, Z_n | \theta)$ \
            $= \sum_n log \Pi_k \pi_kN(x_n | \mu_k, \Sigma)^{I(Z_n =k)}$ \
            $= \sum_n \sum_k I(Z_n =k)log (\pi_kN(x_n | \mu_k, \Sigma))$ \

This expression is easier to solve.

**E step** is to compute:

$P(Z_n = k| x_n, \theta^{old}) = r_{nk} = \frac{\pi_kN(x_n|\mu_k,\Sigma)}{\sum_j \pi_jN(x_n|\mu_j, \Sigma)}$

where $r_{nk}$ is the responsibility of cluster k for data point n


**M step** is to maximizing the expected complete data log likelihood:

$E(l_c(\theta)) = \sum_n P(Z_n|x_n, \theta^{old}) \sum_k I(Z_n =k)log (\pi_kN(x_n | \mu_k, \Sigma))$ \
$= \sum_n \sum_k r_{nk}log(\pi_kN(x_n|\mu_k,\Sigma))$ substitute with $r_{nk}$ \
$= \sum_n \sum_k r_{nk}log\pi_k + \sum_n \sum_k r_{nk}logN(x_n|\mu_k,\Sigma)$ \
$= J(\pi_k) + J(\mu_k, \Sigma)$

Thus, we can optimize $\pi_k$ with objective function $J(\pi)$, optimize $\mu_k$ and $\Sigma$ with objective function $J(\mu_k, \Sigma)$ by taking derivative and setting to zero to solve for all parameters.

From $J(\pi_k)$, we add Lagrange multiplier to ensure $\sum_k \pi_k = 1$, take derivative w.r.t $\pi$ and set to zero to solve for $\pi$.

$\pi_k^{new} = \frac{1}{N} \sum_n r_{nk}$

From $J(\mu_k, \Sigma)$, we expand out the Gaussian, take derivative w.r.t $\mu_k$ and $\Sigma$ separately, set zero to solve for $\mu_k$ and $\Sigma$ respectively.

$\mu_k^{new} = \frac{\sum_n r_{nk}x_n}{\sum_n r_{nk}}$ \


$\Sigma^{new} = \frac{\sum_n \sum_k r_{nk} (x_n - \mu_k^{new})(x_n - \mu_k^{new})^T}{n}$








## Discussion

By following the formula of EM algorithm in lecture with some modification in multi-variate normal pdf and a single sigma for all class, we're able to match with the EM algorithm output of package mclust on faithful dataset.


## Contribution

Vijayakumar Mohan contributes to verification of implementation and derivation of EM formula and overall presentation of the report.

Matthew Leung contributes to implementation and derivation of EM algorithm.
