---
title: "PSL (F20) Coding Assignment 4"
author: "Vijayakumar Sitha Mohan (VS24), Waitong Matthew Leung (wmleung2)"
date: '3-Nov-2020'
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
options(scipen = 1, digits = 7, width = 80, fig.align = "center")
set.seed(8742)
```

## Introduction

Implement the EM algorithm for a p-dimensional Gaussian mixture model with G components:

$\sum_{k=1}^{G} P_i * N(x; \mu_k, \Sigma)$

Store the estimated parameters as a list in R with three components

- prob: G-dimensional probability vector (p1,...,pG)

- mean: p-by-G matrix with the k-th column being $\mu_k$, the p-dimensional mean for the
k-th Gaussian component;

- Sigma: p-by-p covariance matrix $\Sigma$ shared by all G components.


## Prepare my function

We prepare a function to perform the E-step, a function to perform the M-step, and then iteratively call these two functions in myEM.



```{r}

##################################################################
# Public functions
##################################################################
#' Multivariate Normal Density 
#' 
#' Calculates mutlivariate normal densities. This is different to the dmvnorm function 
#' in the mvtnorm package in that it takes a matrix for both x and mean. It then calculates 
#' a vector of densities according to dmvnorm(x[i,],mean[i,],sigma,log = FALSE).
#' To aid computation the mahalanobis distances are calculated in parallel using mclapply.
#' @importFrom parallel mclapply
#' @param x A matrix of values
#' @param mean A matrix of means
#' @param sigma A covariance matrix
#' @param log Boolean for whether we want log densities or not
#' @details My own implementation of the multivariate normal density function
#' for increased efficiency for application in this package
#' because there are so many repeated calls to densitymvnorm using a 
#' given sigma matrix it made sense to have one that could take a matrix 
#' of means as well as a matrix of x's and treat them as 
#' paired, returning the density of x[1,] given mean[1,], x[2,] 
#' given mean[2,] also stops repeated inversions of the matrix sigma, and
#' calculates the densities in parallel
#' @return A vector of densities
#' @export
densityMvNorm = function (x, mean, sigma, log = FALSE) 
{
  ## takes a matrix of means rather than a single vector
  if (missing(sigma)) sigma = diag(ncol(x))
  
  if (NCOL(x) != NCOL(sigma)) {
    stop("x and sigma have non-conforming size")
  }
  if (!isSymmetric(sigma, tol = sqrt(.Machine$double.eps), 
                   check.attributes = FALSE)) {
    stop("sigma must be a symmetric matrix")
  }
  if (NCOL(mean) != NROW(sigma)) {
    stop("mean and sigma have non-conforming size")
  }
  ## invert matrix before hand so only do this once
  prec = solve(sigma)
  means = lapply(1:dim(mean)[1],function(i){mean[i,]})
  #distval = do.call(rbind, 
  #                   mclapply(means, 
  #                            mahalanobis,x = x, cov = prec,inverted = TRUE))
  distval = do.call(rbind, 
                     lapply(means, 
                              mahalanobis,x = x, cov = prec,inverted = TRUE))
  logdet = sum(log(eigen(sigma, symmetric = TRUE, only.values = TRUE)$values))
  logretval = -(ncol(x) * log(2 * pi) + logdet + distval)/2
  
  if (log) 
    return(logretval)
  else 
    return(exp(logretval))
}
```



```{r}

Estep <- function(data, G, para){
  # Your Code
  # Return the n-by-G probability matrix
  
  # initialize pre_r = pi_k * N(x_i|mu_k, sigma) with n x G zeros
  pre_r = matrix(0, nrow(data), G)
  
  # loop through each data point i
  for (i in 1:nrow(data)){
    # loop through each class c
    for (c in 1:G){
      pre_r[i,c] = para$prob[c] * densityMvNorm(data[i,], t(as.matrix(para$mean[,c])), para$Sigma)
    }#c      
  }#i
  
  # calculate responsibility r
  r = matrix(0, nrow(data), G)
  r = pre_r/apply(pre_r, 1, sum)

  # return responsibility r
  r
  
}

Mstep <- function(data, G, para, post.prob){ 
  # Your Code
  # Return the updated parameters

  # update prob
  para$prob = apply(post.prob, 2, mean)
  
  
  mySigma = matrix(0,G,G)
  # loop through all class
  for (c in 1:G){

    # update mean
    para$mean[,c] = apply(data * post.prob[,c], 2, sum)/ sum(post.prob[,c])
       
    # calculate sigma
    # subtract class mean from data and convert to matrix for each data i
    for (i in 1:nrow(data)){
      myDatum = as.matrix(data[i,] - para$mean[,c])
      mySigma = mySigma + post.prob[i,c] * t(myDatum) %*% myDatum
    }#i
    
  }#G
  
  # update Sigma
  para$Sigma = mySigma/nrow(data)
  

  para  
}

myEM <- function(data, itmax, G, para){
  # itmax: num of iterations
  # G:     num of components
  # para:  list of parameters (prob, mean, Sigma)
  for(t in 1:itmax){
    post.prob <- Estep(data, G, para)
    para <- Mstep(data, G, para, post.prob)
  }
  return(para)
}


```



## Test my function

Test my function with data faithful from mclust.


### Load data

```{r}

library(mclust)
dim(faithful)


```


```{r}
head(faithful)

```

```{r}
n <- nrow(faithful)

```


### Initialization

The mixture model in this assignment correponds to modelName = "EEE" in mclust. We initialize parameters by first randomly assigning the n samples into two groups and then running one iteration of the built-in M-step.

```{r}

set.seed(8742)
Z <- matrix(0, n, 2) 
Z[sample(1:n, 120), 1] <- 1 
Z[, 2] <- 1 - Z[, 1]
ini0 <- mstep(modelName="EEE", faithful , Z)$parameters

```

Here are the initial values we use for (prob, mean, Sigma).


```{r}

para0 <- list(prob = ini0$pro, 
              mean = ini0$mean, 
              Sigma = ini0$variance$Sigma)
para0

```



### Compare Results

Compare the estimated parameters returned by myEM and the ones returned by the EM algorithm in mclust after 10 iterations.

- Output from myEM


```{r}

myEM(data=faithful, itmax=10, G=2, para=para0)

```


- Output from mclust


```{r}

Rout <- em(modelName = "EEE", data = faithful,
           control = emControl(eps=0, tol=0, itmax = 10), 
           parameters = ini0)$parameters

list(Rout$pro, Rout$mean, Rout$variance$Sigma)

```



## Discussion

By following the formula of EM algorithm in lecture with some modification in multi-variate normal pdf and a single sigma for all class, we're able to match with the EM algorithm output of package mclust on faithful dataset.


## Contribution

Vijayakumar Mohan contributes to verification of implementation and derivation of EM formula

Matthew Leung contributes to implementation of EM algorithm and overall presentation of the report
