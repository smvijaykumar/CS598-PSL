---
title: "PSL (F20) Project 1"
author: "Vijayakumar Sitha Mohan (VS24), Waitong Matthew Leung (wmleung2)"
date: '13-Sept-2020'
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
set.seed(8742)

library(dplyr)
library(tidyr)
library(caret)
library(tidyr)
library(knitr)
library(xgboost)
library(glmnet)
library(e1071)
```

## Introduction

The goal is to predict the final price of a home (in log scale) using AMES housing dataset. As a project requirement we need to build Two prediction models and we chose Lasso Regression and XBGBoost as our model to predict the sale price.

The dataset has 2930 rows (i.e., houses) and 83 columns.
    The first column is “PID”, the Parcel identification number;
    The last column is the response variable, Sale_Price;
    The remaining 81 columns are explanatory variables describing (almost) every aspect of residential homes.

Finally we need to create two output files with the predictions prices from each model.

## Method

### Utility Functions
```{r echo=FALSE}

hotEncoding = function(dataset) {
  dummies_model = dummyVars(Sale_Price ~ . -PID, data=dataset)
  dataset = predict(dummies_model, newdata = dataset)
  dataset = data.frame(dataset)
}

RMSE = function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}


removeVars = function(data,remove.var) {

  #remove.var <- c('Street', 'Utilities',  'Condition_2', 'Roof_Matl', 'Heating', 'Pool_QC', 'Misc_Feature', 'Low_Qual_Fin_SF', 'Pool_Area', 'Longitude','Latitude')
  data[,!names(data) %in% remove.var]
}

getSkewedVars = function(data) {
  skewedVars <- NA

  for(i in names(data)){
    if(is.numeric(data[,i])){
      if((i != "Sale_Price") & (i != "PID")){
        if(length(levels(as.factor(data[,i]))) > 10){
          # Enters this block if variable is non-categorical
          skewVal <- skewness(data[,i])
          if(abs(skewVal) > 0.25){
            skewedVars <- c(skewedVars, i)
          }
        }
      }
    }
  }
  skewedVars[-1]
}
```

### Data Read and Exploration


- Load Data
```{r}
data = read.csv("Ames_data.csv")
```

- Analysis of Response Variable(Sale Price)

A mean and median sale price of $180,796 and $160,000 respectively, indicating a positive skew, which was confirmed by histogram of the Sale Price.

A long right tail tells that there are outlier data points. max = 755,000 and min = 12,789

- Skewness in Data

```{r echo=FALSE}
par(mfrow=c(1,4))
hist(data$Sale_Price,col = "blue", main = "Histogram of Sale Price", xlab = "Sale Price")

plot(Sale_Price ~ Gr_Liv_Area, data = data)
plot(Sale_Price ~ Garage_Area, data = data)
plot(Sale_Price ~ Lot_Area, data = data)
```

### Data PreProcessing

- Remove features with Zero variance
```{r}
  # Find zero varience factors using caret
  nzv.data = nearZeroVar(data, saveMetrics = TRUE)

  # Remove zero variance factors
  drop.cols = rownames(nzv.data)[nzv.data$nzv == TRUE]
  remove.var = c(drop.cols, "Longitude","Latitude")
  data = removeVars(data,remove.var)
```

- Impute Missing Values with Preprocess using Caret package

```{r}
d <- preProcess(data, "medianImpute")
data = predict(d,data)
```

- Log transformation for skewed features
```{r}
skewed.vars = getSkewedVars(data)
data_encoded = hotEncoding(data)
for(i in skewed.vars){
  if(0 %in% data_encoded[, i]){
    data_encoded[,i] <- log(1+data_encoded[,i])
  }
  else{
    data_encoded[,i] <- log(data_encoded[,i])
  }
}
data_encoded$PID = data$PID
data_encoded$Sale_Price = data$Sale_Price
data_encoded$Sale_Price_Log = log(data$Sale_Price)
```

- Remove Outlier Data points

```{r}
fit = lm(Sale_Price_Log ~ Lot_Area + Mas_Vnr_Area  + Bsmt_Unf_SF + Total_Bsmt_SF + Second_Flr_SF + First_Flr_SF + Gr_Liv_Area + Garage_Area + Wood_Deck_SF   , data = data_encoded)
fit_cd = cooks.distance(fit)
data_encoded = data_encoded[fit_cd < 4 / length(fit_cd),]

```

- Check for missing value

```{r echo=FALSE}
paste("Number of Missing Values", sum(is.na(data_encoded)))
data_encoded <- data_encoded %>% 
                    fill(
                      dplyr::everything()
                    )
# counting number of missing values
paste("Number of Missing Values", sum(is.na(data_encoded)))

```

- Train and Test Data Split
    Split the dataset into Train and Test by using given test project ids.

```{r echo=FALSE}

testIDs <- read.table("project1_testIDs.dat")
j <- 2
train <- data_encoded[-testIDs[,j], ]
test <- data_encoded[testIDs[,j], ]
#test.y <- test[, c(1, 136,137)]
#test <- test[, -c(136,137)]
test.y <- test[, c("PID", "Sale_Price","Sale_Price_Log")]
test <- test[, !names(test) %in% c("Sale_Price","Sale_Price_Log")]

write.csv(train,"train.csv",row.names=FALSE)
write.csv(test, "test.csv",row.names=FALSE)
write.csv(test.y,"test_y.csv",row.names=FALSE)

trainData <- read.csv("train.csv")
testData <- read.csv("test.csv")
testData = testData[complete.cases(testData),]
test.y = test.y[complete.cases(test.y),]

```

### Model Building

- LASSO Regression Model

+ Cross Validation to choose Lamda hyperparameter
```{r}
set.seed(8742)
X = trainData[,!names(trainData) %in% c("Sale_Price","Sale_Price_Log","PID")]
cv_lasso=cv.glmnet(as.matrix(X),trainData$Sale_Price_Log, nfolds = 10, alpha = 1)
cv_lasso$lambda.min
```

+ Feature Engineering
```{r}
# select variables
sel.vars <- predict(cv_lasso, type="nonzero", s = cv_lasso$lambda.min)$X1
```

+ Fit & Predict using selected Model

```{r echo=FALSE}
lasso_mod = cv.glmnet(as.matrix(X[,sel.vars]), trainData$Sale_Price_Log, alpha = 1)

# ## Predictions
preds_train<-predict(lasso_mod,newx=as.matrix(X[,sel.vars]),s=cv_lasso$lambda.min, alpha = 1)
RMSE(trainData$Sale_Price_Log,preds_train)
#trainData$pred_sale_price = preds_train

preds<-predict(lasso_mod,newx=as.matrix(testData[,sel.vars]),s=cv_lasso$lambda.min, alpha = 1)
RMSE(test.y$Sale_Price_Log,preds)
test.y$pred_sale_price = preds
 
```


- XGBoost - Boosting Model
+ Cross Validation to choose Lamda, subsample, max_depth hyperparameter

```{r eval=FALSE, message=FALSE, warning=FALSE, echo=FALSE}
start.time <- Sys.time()

dtrain <- xgb.DMatrix(data = as.matrix(trainData[,!names(trainData) %in% c("Sale_Price","Sale_Price_Log")]), label = as.matrix(trainData$Sale_Price_Log)) 
dtest <- xgb.DMatrix(data = as.matrix(testData), label=test.y$Sale_Price_Log)
registerDoMC(8)
# Create empty lists
lowest_error_list = list()
parameters_list = list()

# Create 10,000 rows with random hyperparameters
set.seed(8742)
for (iter in 1:10000){
  param <- list(booster = "gbtree",
                objective = "reg:squarederror",
                max_depth = sample(4:6, 1),
                eta = runif(1, .01, .05),
                subsample = runif(1, .5, 0.6)
    )
  parameters <- as.data.frame(param)
  parameters_list[[iter]] <- parameters
}

# Create object that contains all randomly created hyperparameters
parameters_df = do.call(rbind, parameters_list)


# Use randomly created parameters to create 10,000 XGBoost-models
for (row in 1:nrow(parameters_df)){
  set.seed(8742)
  mdcv <- xgb.train(data = dtrain, nfold = 5, 
                    booster = "gbtree",
                    objective = "reg:squarederror",
                    max_depth = parameters_df$max_depth[row],
                    eta = parameters_df$eta[row],
                    subsample = parameters_df$subsample[row],
                    nrounds= 1000,
                    eval_metric = "rmse",
                    early_stopping_rounds= 30,
                    print_every_n = 100,
                    watchlist = list(train= dtrain, val= dtest)
  )
  lowest_error <- as.data.frame(1 - min(mdcv$evaluation_log$val_error))
  lowest_error_list[[row]] <- lowest_error
}

# Create object that contains all accuracy's
lowest_error_df = do.call(rbind, lowest_error_list)

# Bind columns of accuracy values and random hyperparameter values
randomsearch = cbind(lowest_error_df, parameters_df)

# Quickly display highest accuracy
max(randomsearch$`1 - min(mdcv$evaluation_log$val_error)`)

# Stop time and calculate difference
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```

+ Fit & Predict using selected Model

```{r echo=FALSE, message=FALSE, warning=FALSE}
set.seed(8742)
xgbFit = xgboost(data = as.matrix(trainData[,!names(trainData) %in% c("Sale_Price","Sale_Price_Log","PID")]), label = as.matrix(trainData$Sale_Price_Log), 
    nrounds = 2200, verbose = FALSE, objective = "reg:squarederror", eval_metric = "rmse", 
    nthread = 8, eta = 0.01, max_depth = 6, 
    subsample = 0.5213)

## Predictions
# rmse of training data
predict_xg_train = predict(xgbFit, newdata = as.matrix(trainData[,!names(trainData) %in% c("Sale_Price","Sale_Price_Log","PID")]))
RMSE(trainData$Sale_Price_Log, predict_xg_train)
# rmse of testing data
preds2 <- predict(xgbFit, newdata = as.matrix(testData[,!names(testData) %in% c("PID")]))
RMSE(test.y$Sale_Price_Log, preds2)
test.y$pred_sale_price = round(exp(preds2),1)
mysubm2 = data.frame(test.y[,c("PID","pred_sale_price")])
colnames(mysubm2) = c("PID","Sale_Price")
write.csv(mysubm2, file = "mysubmission2.txt", row.names = FALSE)

```

## Results


```{r echo=FALSE}
set.seed(8742)

n = 10
nr = nrow(trainData)
lasso.train.error = rep(0,n)
lasso.test.error = rep(0,n)
#train.10 = split(trainData, f = rep_len(1:n,nr))
train.10 = split(trainData, sample(1:n, nr, replace =T ))
for(i in 1:length(train.10)) {
  trainData1 = train.10[[i]]
  X = trainData1[,!names(trainData1) %in% c("Sale_Price","Sale_Price_Log","PID")]
  preds_train<-predict(lasso_mod,newx=as.matrix(X[,sel.vars]),s=cv_lasso$lambda.min, alpha = 1)
  lasso.train.error[i] = (RMSE(trainData1$Sale_Price_Log,preds_train))
}
nr = nrow(test.y)
test1Data = merge(testData, test.y, by = "PID")
test.y.10 = split(test1Data, sample(1:n, nr, replace = TRUE ))
#test.y.10 = split(test1Data, f= rep_len(1:n,nr))
for(i in 1:length(test.y.10)) {
  testData1 = test.y.10[[i]]
  X = testData1[,!names(testData1) %in% c("Sale_Price","Sale_Price_Log","PID","pred_sale_price")]
  preds_test<-predict(lasso_mod,newx=as.matrix(X[,sel.vars]),s=cv_lasso$lambda.min, alpha = 1)
  lasso.test.error[i] = (RMSE(testData1$Sale_Price_Log,preds_test))
}

```


```{r echo=FALSE}
set.seed(8742)
n = 10
nr = nrow(trainData)
xg.train.error = rep(0,n)
xg.test.error = rep(0,n)
#train.10 = split(trainData, f = rep_len(1:n,nr))
train.10 = split(trainData, sample(1:n, nr, replace =T ))
for(i in 1:length(train.10)) {
  trainData1 = train.10[[i]]
  X = trainData1[,!names(trainData1) %in% c("Sale_Price","Sale_Price_Log","PID")]
  preds_train = predict(xgbFit, newdata = as.matrix(X))
  xg.train.error[i] = (RMSE(trainData1$Sale_Price_Log,preds_train))
}
nr = nrow(test.y)
test1Data = merge(testData, test.y, by = "PID")
test.y.10 = split(test1Data, sample(1:n, nr, replace = TRUE ))
#test.y.10 = split(test1Data, f= rep_len(1:n,nr))
for(i in 1:length(test.y.10)) {
  testData1 = test.y.10[[i]]
  X = testData1[,!names(testData1) %in% c("Sale_Price","Sale_Price_Log","PID","pred_sale_price")]
  preds_test <- predict(xgbFit, newdata = as.matrix(X))
  xg.test.error[i] = (RMSE(testData1$Sale_Price_Log,preds_test))
}

```

```{r echo=FALSE}
results = data.frame(cbind(lasso.train.error,lasso.test.error,xg.train.error,xg.test.error))
colnames(results)=c("Lasso Train Error","Lasso Test Error","XGBoost Train Error","XGBoost Test Error")
results
```


## Discussion

- There are two important observations we had during model building.

  1. Without outlier removal Lasso Model didn't meet the criteria specified by the project requirement. When we handled the outlier data points by doing linear regression and using cooks distiance metrics then the performance was greatly improved and both train and test errors for Lasso Regression was below 10.
  
  2. The hyperparameter tuning for XGBoost runs too long and to tune Lambda, MaX_Depth and SubSample, took 11 hours to find the optimal value using my laptop.
