---
title: "PSL (F20) Project 2"
author: "Vijayakumar Sitha Mohan (VS24), Waitong Matthew Leung (wmleung2)"
output:
  pdf_document: default
  html_document: 
    theme: readable
    toc: no
urlcolor: cyan
fontsize: 10pt
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```


```{r echo = FALSE, message=FALSE, warning=FALSE}
library(lubridate)
library(tidyverse)
library(dplyr)
library(tidyr)
library(kableExtra)
```

#### Introduction


The goal is to predict Walmart Store Sales Price for the given train_ini.csv, the data till 2011-02, you need to predict the weekly sales for 2011-03 and 2011-04. Then youâ€™ll be provided with the weekly sales data for 2011-03 and 2011-04 (fold_1.csv), and you need to predict the weekly sales for 2011-05 and 2011-06, and so on.


#### Overall Approach


Our approach is to train a linear model for each Store, Dept combination. We use the historical weekly sales data to predict future Weekly_Sales (response variable Y) by Store/Dept combination. 

The idea is to predict each week by the same week from last year for each combination of Store/Dept. Since we train each linear model with only data from the same Store and Dept, we don't need to include Store/Dept in the predictor. Since we predict each week by the same week from last year, if this week is holiday week, this week last year was also holiday week. Thus, the linear model doesn't need to include IsHoliday as predictor. 

We derive two predictors, namely "Yr" and "Wk", from original data column Date. "Wk" is the number of the week in each year starting from 1 to 52(or 53). "Yr" is the year from data column Date. We train a linear regression model: sqrt(Y) ~ Yr + Wk for each combination of Store/Dept. We apply square root transformation to response variable Y to reduce skewness of data. It inflates smaller Y but stabilizes bigger Y. 



#### Data PreProcessing

We preform the following data preprocesssing before model training:

- replace any negative response variable Weekly_Sales with zero

- extract year to predictor "Yr" from Date

- extract week to predictor "Wk" from Date

- check if "Yr" == 2010, subtract 1 from "Wk" since there is one more week in 2010. This is to line up the week from last year. The most important thing is to line up holiday weekend as a result.

- set "Wk" as factor variable with 52 levels, i.e. one per week

- since there is some missing data, we use model.matrix() to build design matrix and replace any NA with zero before feeding to lm() to prevent erroring out lm()



```{r echo=FALSE, message=FALSE, warning=FALSE}
# read raw data and extract date column
train_raw <- readr::read_csv(unz('train.csv.zip', 'train.csv'))
train_dates <- train_raw$Date

# training data from 2010-02 to 2011-02
start_date <- ymd("2010-02-01")
end_date <- start_date %m+% months(13)

# split dataset into training / testing
train_ids <- which(train_dates >= start_date & train_dates < end_date)
train = train_raw[train_ids, ]
test = train_raw[-train_ids, ]

# create the initial training data
readr::write_csv(train, 'train_ini.csv')

# create test.csv
# removes weekly sales
test %>%
  select(-Weekly_Sales) %>%
  readr::write_csv('test.csv')

# create 10-fold time-series CV
num_folds <- 10
test_dates <- train_dates[-train_ids]

# month 1 --> 2011-03, and month 20 --> 2012-10.
# Fold 1 : month 1 & month 2, Fold 2 : month 3 & month 4 ...
for (i in 1:num_folds) {
    # filter fold for dates
    start_date <- ymd("2011-03-01") %m+% months(2 * (i - 1))
    end_date <- ymd("2011-05-01") %m+% months(2 * (i - 1))
    test_fold <- test %>%
        filter(Date >= start_date & Date < end_date)
   
    # write fold to a file
    readr::write_csv(test_fold, paste0('fold_', i, '.csv'))
}


```

```{r echo = FALSE, message=FALSE, warning=FALSE}
t = 1
train <- readr::read_csv('train_ini.csv')
test <- readr::read_csv('test.csv')

```


```{r echo = FALSE, message=FALSE, warning=FALSE}
mypredict = function(){
  if (t>1){
    train <<- train %>% add_row(new_train)
  }
 
  start_date <- ymd("2011-03-01") %m+% months(2 * (t - 1))
  end_date <- ymd("2011-05-01") %m+% months(2 * (t - 1))
 
  test_current <- test %>%
    filter(Date >= start_date & Date < end_date) %>%
    select(-IsHoliday)
  
  # Convert negative Weekly Sales to 0
  train$Weekly_Sales[train$Weekly_Sales < 0] = 0
  #train$Weekly_Sales[train$Weekly_Sales == 0] = 1
 

# not all depts need prediction
  test_depts <- unique(test_current$Dept)
  test_pred <- NULL
 
  for(dept in test_depts){
    train_dept_data <- train %>% filter(Dept == dept)
    test_dept_data <- test_current %>% filter(Dept == dept)
   
   # no need to consider stores that do not need prediction
   # or do not have training samples
    train_stores <- unique(train_dept_data$Store)
    test_stores <- unique(test_dept_data$Store)
    test_stores <- intersect(train_stores, test_stores)
   
    for(store in test_stores){
      tmp_train <- train_dept_data %>%
        filter(Store == store) %>%
        mutate(Wk = ifelse(year(Date) == 2010, week(Date)-1, week(Date))) %>%
        mutate(Yr = year(Date))
      tmp_test <- test_dept_data %>%
        filter(Store == store) %>%
        mutate(Wk = ifelse(year(Date) == 2010, week(Date)-1, week(Date))) %>%
        mutate(Yr = year(Date))

      tmp_train$Wk = factor(tmp_train$Wk, levels = 1:52)
      tmp_test$Wk = factor(tmp_test$Wk, levels = 1:52)
      train_model_matrix <- model.matrix(~ Yr + Wk, tmp_train)
      test_model_matrix <- model.matrix(~ Yr + Wk, tmp_test)
     

   
      mycoef <- lm(sqrt(tmp_train$Weekly_Sales) ~ train_model_matrix)$coef
      mycoef[is.na(mycoef)] <- 0
      tmp_pred <- mycoef[1] + test_model_matrix %*% mycoef[-1]
      tmp_pred = tmp_pred^2
 
      tmp_test <- tmp_test %>%
        mutate(Weekly_Pred = tmp_pred[,1]) %>%
        select(-Wk, -Yr)
      test_pred <- test_pred %>% bind_rows(tmp_test)
   }
  }
  return(test_pred)
}
```



```{r echo=FALSE, message=FALSE, warning=FALSE}
# save weighted mean absolute error WMAE
start.time = Sys.time()
num_folds <- 10
wae <- rep(0, num_folds)

for (t in 1:num_folds) {
  # *** THIS IS YOUR PREDICTION FUNCTION ***
  test_pred <- mypredict()
 
  # load fold file
  fold_file <- paste0('fold_', t, '.csv')
  new_train <- readr::read_csv(fold_file,
                               col_types = cols())

  # extract predictions matching up to the current fold
  scoring_tbl <- new_train %>%
      left_join(test_pred, by = c('Date', 'Store', 'Dept'))
  #readr::write_csv(scoring_tbl, paste0('scoring_', t, '.csv'))
  # compute WMAE
  actuals <- scoring_tbl$Weekly_Sales
  preds <- scoring_tbl$Weekly_Pred
  preds[is.na(preds)] <- 0
  weights <- if_else(scoring_tbl$IsHoliday, 5, 1)
  wae[t] <- sum(weights * abs(actuals - preds)) / sum(weights)
}

end.time = Sys.time()

```




#### Results


- 10-Fold Weighed Mean Absolute Error (WMAE)


```{r echo=FALSE, message=FALSE, warning=FALSE}
results = data.frame(Fold=seq(1,10), WeightedMeanError=wae)
results1 = data.frame(Mean_WAE=mean(wae))
knitr::kable(results) %>% kable_styling(bootstrap_options = "striped")

```


- Mean WMAE

```{r echo=FALSE, message=FALSE, warning=FALSE}

knitr::kable(results1) %>% kable_styling(bootstrap_options = "striped")

```




- Running time: System: MSI laptop, Intel i5, 2.0GHz, 8GB, Win 10

```{r echo=FALSE, message=FALSE, warning=FALSE}
results1 = data.frame(Run.Time=(end.time-start.time))
knitr::kable(results1) %>% kable_styling(bootstrap_options = "striped", font_size = 10)
```

#### Discussion

We made couple of changes from Professor suggested approach in order to meet the bench mark WMAE, namely to handle negative Weekly_Sales and square root response variable transformation.

We tried to replace negative Weekly_Sales with 1 but WMAE got worse. We settled with replacing by 0 which improve WMAE.

We tried log response variable transformation but WMAE got worse. We settled with square root response variable transformation.