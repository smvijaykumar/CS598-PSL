---
title: "PSL (F20) Project 3"
author: "Vijayakumar Sitha Mohan (VS24), Waitong Matthew Leung (wmleung2)"
date: '15-Nov-2020'
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
library(dplyr) 
library(tidyr) 
library(reshape2)
library(tidyverse)
library(glmnet)
library(randomForest)
library(xgboost)
library(pROC)
library(text2vec)
library(tm)
library(slam)
```

```{r eval=FALSE}
data <- read.table("/Users/vsitham/Downloads/project3/alldata.tsv", stringsAsFactors = FALSE, header = TRUE)
testIDs <- read.csv("splits_F20.csv", header = TRUE)
for(j in 1:5){
  dir.create(paste("split_", j, sep=""))
  train <- data[-testIDs[,j], c("id", "sentiment", "review") ]
  test <- data[testIDs[,j], c("id", "review")]
  test.y <- data[testIDs[,j], c("id", "sentiment", "score")]
  
  tmp_file_name <- paste("split_", j, "/", "train.tsv", sep="")
  write.table(train, file=tmp_file_name, 
              quote=TRUE, 
              row.names = FALSE,
              sep='\t')
  tmp_file_name <- paste("split_", j, "/", "test.tsv", sep="")
  write.table(test, file=tmp_file_name, 
              quote=TRUE, 
              row.names = FALSE,
              sep='\t')
  tmp_file_name <- paste("split_", j, "/", "test_y.tsv", sep="")
  write.table(test.y, file=tmp_file_name, 
            quote=TRUE, 
            row.names = FALSE,
            sep='\t')
}
```

```{r}
stop_words = c("i", "me", "my", "myself", 
               "we", "our", "ours", "ourselves", 
               "you", "your", "yours", 
               "their", "they", "his", "her", 
               "she", "he", "a", "an", "and",
               "is", "was", "are", "were", 
               "him", "himself", "has", "have", "who","when","where","why",
               "it", "its", "the", "us")
stpw2 = tm::stopwords("en")
#stpw2 = stopwords::stopwords(language = "en",source = "nltk")                   # tm package stop word list; tokenizer package has the same name function
#comn  = unique(c(stpw1, stpw2))                 # Union of two list
#stop_words = unique(gsub("'"," ",comn))  # final stop word lsit after removing punctuation

```


```{r message=FALSE, warning=FALSE}
```


```{r message=FALSE, warning=FALSE}
j = 4
setwd(paste("split_", j, sep=""))

train = read.table("train.tsv",
                   stringsAsFactors = FALSE,
                   header = TRUE)
 #train$review = stringr::str_trunc(train$review, 1000)
 #train$review = gsub('[[:digit:]]+', '', train$review)
 train$review <- gsub('<.*?>', ' ', train$review)
 #train$review = tolower(train$review)
 #train$review = str_replace_all(train$review, "[^[:alnum:]]", " ")
 #train$review = str_replace_all(train$review, "[[:punct:]]", " ")
 #train$review = gsub('\\b\\w{1,2}\\s','',train$review)
 
 
 
 it_train = itoken(train$review,
                    preprocessor = tolower, 
                    tokenizer = word_tokenizer)
 

 
 tmp.vocab = create_vocabulary(it_train, 
                              stopwords = stop_words, 
                              ngram = c(1L,4L))

 tmp.vocab = prune_vocabulary(tmp.vocab, term_count_min = 10,
                             doc_proportion_max = 0.5,doc_proportion_min = 0.001)

 vectorizer = vocab_vectorizer(tmp.vocab)
 #vectorizer = hash_vectorizer(2 ^ 18, c(1L, 2L))
 
 dtm_train = create_dtm(it_train, vectorizer)
 
 #dtm_train = normalize(dtm_train, "l1")
 
 # define tfidf model
 # tfidf = TfIdf$new(smooth_idf = TRUE)
  # fit model to train data and transform train data with fitted model
#  dtm_train_tfidf = fit_transform(dtm_train, tfidf)

```


```{r}
set.seed(8742)

 tmpfit = glmnet(x = dtm_train, 
                y = train$sentiment, 
                alpha = 1,
                family='binomial')
 srt = sort(tmpfit$df[tmpfit$df < 2000],decreasing = TRUE)
 myvocab = colnames(dtm_train)[which(tmpfit$beta[,which(tmpfit$df==srt[1])] != 0)]
 #write.csv(myvocab,file="myvocab.txt",quote = FALSE)
 write_lines(myvocab,"myvocab.txt")
```



```{r}
 #write.csv(myvocab2,file="myvocab.txt",quote = FALSE)
 #myvocab <- scan(file = "myvocab.txt", what = character())
 vectorizer = vocab_vectorizer(create_vocabulary(myvocab,ngram = c(1L,2L)))
 
 dtm_train = create_dtm(it_train, vectorizer)
```



```{r}
v.size = dim(dtm_train)[2]
ytrain = train$sentiment
summ = matrix(0,nrow=v.size,ncol = 4)
summ[,1] = colapply_simple_triplet_matrix(as.simple_triplet_matrix(dtm_train[ytrain==1,]),mean)
summ[,2] = colapply_simple_triplet_matrix(as.simple_triplet_matrix(dtm_train[ytrain==1,]),var)
summ[,3] = colapply_simple_triplet_matrix(as.simple_triplet_matrix(dtm_train[ytrain==0,]),mean)
summ[,4] = colapply_simple_triplet_matrix(as.simple_triplet_matrix(dtm_train[ytrain==0,]),var)
n1 = sum(ytrain)
n = length(ytrain)
n0 = n - n1
myp = (summ[,1] - summ[,3]) / sqrt(summ[,2]/n1 + summ[,4]/n0)

```

```{r}
words = colnames(dtm_train)
id = order(abs(myp),decreasing=TRUE)[1:2000]
pos.list = words[id[myp[id] > 0]]
neg.list = words[id[myp[id] < 0]]
myvocab2 = c(pos.list,neg.list)
#write.csv(myvocab2, file="myvocab2.txt",quote = FALSE)
write_lines(myvocab2,"myvocab.txt")
```

```{r}

vectorizer = vocab_vectorizer(create_vocabulary(myvocab2,ngram = c(1L,2L)))
 
dtm_train = create_dtm(it_train, vectorizer)
```

```{r}
 tmpfit = glmnet(x = dtm_train, 
                y = train$sentiment, 
                alpha = 1,
                family='binomial')
 srt = sort(tmpfit$df[tmpfit$df < 1000],decreasing = TRUE)
 myvocab = colnames(dtm_train)[which(tmpfit$beta[,which(tmpfit$df==srt[1])] != 0)]
 write_lines(myvocab,"myvocab.txt")
```


```{r}
 myvocab <- scan(file = "myvocab.txt", what = character())
 vectorizer = vocab_vectorizer(create_vocabulary(myvocab,ngram = c(1L,2L)))
 
 dtm_train = create_dtm(it_train, vectorizer)

 mylogit.cv = cv.glmnet(x = dtm_train, 
                         y = train$sentiment, 
                         alpha = 0,
                         family='binomial', 
                         type.measure = "auc")
 mylogit.fit = glmnet(x = dtm_train, 
                       y = train$sentiment, 
                       alpha = 0,
                       lambda = mylogit.cv$lambda.min, 
                       family='binomial')

```



```{r}

for(j in 1:5) {
 path = paste("C:/Users/Ranjini/GitHub/CS598-PSL/CS598-PSL/project3/split_", j, sep="")
 setwd(path)
 test = read.table("test.tsv",
                    stringsAsFactors = FALSE,
                    header = TRUE)
 test$review <- gsub('<.*?>', ' ', test$review)
 #test$review = tolower(test$review)
 #test$review = str_replace_all(test$review, "[^[:alnum:]]", " ")
 #test$review = str_replace_all(test$review, "[[:punct:]]", " ")
 #test$review = gsub('\\b\\w{1,2}\\s','',test$review)
 #test$review = gsub('[[:digit:]]+', '', test$review)

 it_test = itoken(test$review,
                   preprocessor = tolower, 
                   tokenizer = word_tokenizer)

 dtm_test = create_dtm(it_test, vectorizer)
 
 mypred = predict(mylogit.fit, dtm_test, type = "response")
 output = data.frame(id = test$id, prob = as.vector(mypred))
 write.table(output, file = "mysubmission.txt", 
            row.names = FALSE, sep='\t')
  
 test.y = read.table("test_y.tsv", header = TRUE)
 pred = read.table("mysubmission.txt", header = TRUE)
 pred = merge(pred, test.y, by="id")
 roc_obj = roc(pred$sentiment, pred$prob)
 tmp = pROC::auc(roc_obj)
 print(round(tmp,2))
}
```

```{r}

```

